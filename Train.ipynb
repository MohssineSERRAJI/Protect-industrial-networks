{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEE4YEu5D8K7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "882061f7-8b8d-4858-ed07-3be70c201cbd"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = 'drive/MyDrive/Colab Notebooks/TER_DATA/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy7f6aPzEGya"
      },
      "source": [
        "# **Data Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7EmhmM2EPDq"
      },
      "source": [
        "This dataset was created by the IXIA PerfectStorm tool in the Cyber Range Lab of the Australian Centre for Cyber Security (ACCS) for generating a hybrid of real modern normal activities and synthetic contemporary attack behaviours\n",
        "<a href='https://www.kaggle.com/mrwellsdavid/unsw-nb15'>more information</a>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP_eKlNMEKo_"
      },
      "source": [
        "import os\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv )\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.callbacks import ModelCheckpoint, Callback\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns # Data vizualisation\n",
        "from sklearn.preprocessing import MinMaxScaler # Data preprocessing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV # For splitting data\n",
        "import pickle # save and load python objects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irGU42ZH6cc8"
      },
      "source": [
        "# ***Load Data From Disk***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRJppFWJEriC"
      },
      "source": [
        "# Load features names types \n",
        "features_df = pd.read_csv(path+'/NUSW-NB15_features.csv', encoding='cp1252')\n",
        "features_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oSpDV87FCl5"
      },
      "source": [
        "# Load Data\n",
        "data = pd.concat(\n",
        "      [\n",
        "      pd.read_csv(path+'/UNSW-NB15_1.csv', header=None),\\\n",
        "      pd.read_csv(path+'/UNSW-NB15_2.csv', header=None),\\\n",
        "      pd.read_csv(path+'/UNSW-NB15_3.csv', header=None),\\\n",
        "      pd.read_csv(path+'/UNSW-NB15_4.csv', header=None)\\\n",
        "      ], ignore_index=True)\n",
        "features_name = features_df['Name'].to_numpy()# get feautures names\n",
        "data.columns = features_name# add columns names\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjCqrzFV5-mM"
      },
      "source": [
        "# **Plotting The History Of  Accuracy And Loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP9_quFP59eT"
      },
      "source": [
        "def plot_training_val_loss(H, N, plotPath): \n",
        "  \"\"\"\n",
        "    This function used to plot the progress of the loss during the \n",
        "    training.\n",
        "      args:\n",
        "        H: history of train\n",
        "        N: number of epochs\n",
        "        plotPath: The path of the image that will be saved\n",
        "  \"\"\"  \n",
        "  plt.style.use(\"ggplot\")\n",
        "  plt.figure()\n",
        "  plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "  plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "  plt.plot(np.argmax(H.history[\"loss\"]), np.min(H.history[\"loss\"]), marker=\"x\",color=\"r\", label=\"meilleur train_loss\")\n",
        "  plt.title(\"Training & Val loss\")\n",
        "  plt.xlabel(\"Epoch #\")\n",
        "  plt.ylabel(\"loss\")\n",
        "  plt.legend(loc=\"lower left\")\n",
        "  plt.savefig(plotPath)\n",
        "  plt.show()\n",
        "\n",
        "def plot_training_val_accu(H, N, plotPath):   \n",
        "  \"\"\"\n",
        "    This function used to plot the progress of the accuracy during the \n",
        "    training.\n",
        "      args:\n",
        "        H: history of train\n",
        "        N: number of epochs\n",
        "        plotPath: The path of the image that will be saved\n",
        "  \"\"\"  \n",
        "  plt.style.use(\"ggplot\")\n",
        "  plt.figure()\n",
        "  plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "  plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "  plt.plot(np.argmax(H.history[\"val_accuracy\"]), np.max(H.history[\"val_accuracy\"]), marker=\"x\",color=\"g\", label=\"meilleur accuracy\")\n",
        "  plt.title(\"Training & Val Accuracy\")\n",
        "  plt.xlabel(\"Epoch #\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend(loc=\"lower left\")\n",
        "  plt.savefig(plotPath)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBRLMKBo6oEj"
      },
      "source": [
        "# ***Data Preprocessing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNCHIAD0zx4B"
      },
      "source": [
        "def data_preprocessing(data):\n",
        "  \"\"\"\n",
        "    This function used preprocess the data and prepared for training. \n",
        "      args:\n",
        "        data: The data to process\n",
        "      Return:\n",
        "        processed data\n",
        "        unlabled_data: The data features\n",
        "        y: The data labels\n",
        "  \"\"\"  \n",
        "  print(\"Data preprocessing ...\")\n",
        "  # Delete 'srcip', 'sport', 'dstip', 'dsport' columns\n",
        "  data = data.drop(['srcip', 'sport', 'dstip', 'dsport'], axis=1)\n",
        "  # replace spaces by 0\n",
        "  data['ct_ftp_cmd'] = data['ct_ftp_cmd'].replace(' ', 0)\n",
        "  # Change dtype to int64 of ct_ftp_cmd feauture\n",
        "  data = data.astype({'ct_ftp_cmd': 'int64'})\n",
        "\n",
        "  ##### Handle missing values #####\n",
        "  # Fill nan values for attack_cat feauture with Normal\n",
        "  data = data.fillna({'attack_cat': 'Normal'})\n",
        "  data['attack_cat'] = data['attack_cat'].replace(' Fuzzers','Fuzzers')\n",
        "  data['attack_cat'] = data['attack_cat'].replace(' Fuzzers ','Fuzzers')\n",
        "  data['attack_cat'] = data['attack_cat'].replace(' Reconnaissance','Reconnaissance')\n",
        "  data['attack_cat'] = data['attack_cat'].replace(' Reconnaissance ','Reconnaissance')\n",
        "  data['attack_cat'] = data['attack_cat'].replace(' Shellcode','Shellcode')\n",
        "  data['attack_cat'] = data['attack_cat'].replace(' Shellcode ','Shellcode')\n",
        "  data['attack_cat'] = data['attack_cat'].replace('Backdoors','Backdoor')\n",
        "  # Fill nan values with 0\n",
        "  data = data.fillna(0) \n",
        "\n",
        "  ##### Encode categorial variables with --> Integer Encoding #####\n",
        "  # Get list of labels categories\n",
        "  proto_list = data['proto'].unique().tolist()\n",
        "  state_list = data['state'].unique().tolist()#unique() function return ndarray object\n",
        "  service_list = data['service'].unique().tolist()\n",
        "  # Apply Integer Encoding \n",
        "  data['proto'] = data['proto'].apply(lambda x: proto_list.index(x))\n",
        "  data['state'] = data['state'].apply(lambda x: state_list.index(x))\n",
        "  data['service'] = data['service'].apply(lambda x: service_list.index(x))\n",
        "  # Encode The Class  \n",
        "  attacks_list = data['attack_cat'].unique().tolist()\n",
        "  data['attack_cat'] = data['attack_cat'].apply(lambda x: attacks_list.index(x))\n",
        "  # Returned objects\n",
        "  unlabled_data = data.loc[:, :'ct_dst_src_ltm']\n",
        "  y = data['attack_cat']\n",
        "  print(\"Done !!!\")\n",
        "  return (unlabled_data, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TBRFYD11hjm"
      },
      "source": [
        "def data_normalisation_encode(data, y):\n",
        "  \"\"\"\n",
        "      This function used to normalize the data. \n",
        "      args:\n",
        "        data: The data features\n",
        "        y: The data labels\n",
        "      Return:\n",
        "        processed data\n",
        "        transformed_data: The normalized features\n",
        "        transformed_y: The normalized labels\n",
        "  \"\"\"\n",
        "  print(\"Starting normalisation ...\")\n",
        "  #Normalisation of data\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(data)# Compute the minimum and maximum to be used for later scaling.\n",
        "  transformed_data = pd.DataFrame(scaler.fit_transform(data))\n",
        "  #Encode data to_categorical from keras\n",
        "  transformed_y = tf.keras.utils.to_categorical(y, num_classes=10)\n",
        "  print(\"Normalisation done !!!\")\n",
        "  return (transformed_data, transformed_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRoMyRP22Z_c",
        "outputId": "d76731de-c409-4ed6-b065-9510aa71b219"
      },
      "source": [
        "x, y = data_preprocessing(data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data preprocessing ...\n",
            "Done !!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "713edj0q2d9D",
        "outputId": "2d76b7f1-5543-480f-fd28-2d752ff2ed6d"
      },
      "source": [
        "x, y = data_normalisation_encode(x, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting normalisation ...\n",
            "Normalisation done !!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e5Dj1l23YjR"
      },
      "source": [
        "## **Splitting Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXx17je8f-Zi"
      },
      "source": [
        "*In this stage we split the data to 70% train and 30% test*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HitPWy103Kyp"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test  = train_test_split(x, y, test_size = 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40pwl0sF4A1E"
      },
      "source": [
        "# **Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhgOWvq6gav3"
      },
      "source": [
        "## **First model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhmvl8Q-4CEp"
      },
      "source": [
        "def define_model():\n",
        "  \"\"\"\n",
        "    This function used to define our first model\n",
        "  \"\"\"\n",
        "  model = Sequential()\n",
        "  model.add(Dense(512, input_shape=(43,))) #(784,) is not a typo -- that represents a 784 length vector!\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(10))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.005), metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhOQnV0h45xy"
      },
      "source": [
        "filepath=\"LPT-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model = define_model()\n",
        "history  = model.fit(X_train, Y_train, epochs=50, batch_size=2048, validation_data=(X_test, Y_test), callbacks=callbacks_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fckme4Dd6ciy"
      },
      "source": [
        "plot_training_val_loss(history, 1, \"losss\")\n",
        "plot_training_val_accu(history, 1, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpTHLPsa6oIE"
      },
      "source": [
        "# **Resume The Training Process**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BownbnZN6xw7"
      },
      "source": [
        "*This cell used to resume the training process from .h5 file*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkKw_nbg61Dt"
      },
      "source": [
        "# Util function to get the initial epoch number from the checkpoint name\n",
        "def get_init_epoch(checkpoint_path):\n",
        "    \"\"\"\n",
        "      This function used to get the last epchos\n",
        "      args:\n",
        "        checkpoint_path: A file that will be used to resume the training.\n",
        "      return:\n",
        "        epoch: integer the echop number\n",
        "    \"\"\"\n",
        "    filename = os.path.basename(checkpoint_path)\n",
        "    filename = os.path.splitext(filename)[0]\n",
        "    init_epoch = filename.split(\"-\")[1]\n",
        "    return int(init_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR6odT6q6-GI"
      },
      "source": [
        "# Resume the training process\n",
        "file_path = 'LPT-47-0.0087.h5'\n",
        "model = load_model(file_path)\n",
        "filepath=\"drive/MyDrive/Colab Notebooks/TER_DATA/LPT-{epoch:02d}-{loss:.4f}.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "initial_epoch = get_init_epoch(file_path)\n",
        "history = model.fit(X_train, Y_train, epochs=50, batch_size=2048, validation_data=(X_test, Y_test), callbacks=callbacks_list, initial_epoch=initial_epoch)\n",
        "model.save('drive/MyDrive/Colab Notebooks/TER_DATA/best_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APojFOgugjBB"
      },
      "source": [
        "## **Second model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6M8EF6BhIWt"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten , Activation, SimpleRNN, LSTM, GRU, Dropout, TimeDistributed, Reshape, Input, Lambda, Add\n",
        "from tensorflow.keras import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "def define_model():\n",
        "  \"\"\"\n",
        "    This function used to define our second model\n",
        "  \"\"\"\n",
        "  model = Sequential()\n",
        "  model.add(Reshape((-1,1), input_shape=(43,)))\n",
        "  model.add(Conv1D(32, 3, activation='relu', padding='causal'))\n",
        "  model.add(Conv1D(64, 3, activation='relu', padding='causal'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(LSTM(70, recurrent_dropout=0.1))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  model.compile(optimizer=keras.optimizers.Adam(lr=0.005), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKUQutMghwOS"
      },
      "source": [
        "model = define_model()\n",
        "filepath=\"LPT-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "history  = model.fit(X_train, Y_train, epochs=10, batch_size=2048, validation_data=(X_test, Y_test), callbacks=callbacks_list)\n",
        "model.save('first_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQqm1xYvhaDL"
      },
      "source": [
        "## **Grid search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6XyoU-whd1u"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "\n",
        "activation = [ \"relu\", \"tanh\"]\n",
        "optimzer = [\"SGD\", \"RMSprop\", \"Adam\"]\n",
        "epochs = [ 20 ,30 , 40, 50]\n",
        "batch_size = [2048, 1000]\n",
        "\n",
        "model = KerasClassifier(build_fn=define_model, verbose=1)\n",
        "\n",
        "param_grid = dict(optimzer= [\"Adam\"], epochs=[20], batch_size=batch_size, activation=activation)\n",
        "\n",
        "#use the grid search\n",
        "grid = GridSearchCV(estimator= model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "\n",
        "#start the training\n",
        "grid_result = grid.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}